{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mylib as ml\n",
    "\n",
    "train = pd.read_csv('DATA/Entrenamieto_ECI_2020.csv')\n",
    "test = pd.read_csv('DATA/Validacion_ECI_2020.csv')\n",
    "o_train = train.copy()\n",
    "o_test = test.copy()\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Analizador:\n",
    "\n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "    def get_cols_date(self):\n",
    "        return [x for x in train.columns if \"Date\" in x]\n",
    "\n",
    "    def print_duplicates_count(self):\n",
    "        print(\"Cantidad de duplicados\")\n",
    "        print(len(self.train)-len(self.train.drop_duplicates()))\n",
    "        print(len(self.test)-len(self.test.drop_duplicates()))\n",
    "        print(\"No hay duplicados\")\n",
    "\n",
    "    def eliminar_columnas_que_no_cambian(self):\n",
    "        print('Eliminia las columnas de [test y train] que no aportan informacion en train.')\n",
    "        non_variant_cols = self.train.columns[train.nunique() <= 1]\n",
    "        print(non_variant_cols)\n",
    "        train.drop(non_variant_cols, axis=1, inplace=True)\n",
    "        test.drop(non_variant_cols, axis=1, inplace=True)\n",
    "\n",
    "    def eliminar_all_outliers(self, n=4):\n",
    "        for c in self.train.select_dtypes('number').columns:\n",
    "            self.eliminar_outliers(c, n)\n",
    "\n",
    "    def eliminar_outliers(self, col, n=4):\n",
    "        self.train = self.train[np.abs(self.train[col]-self.train[col].mean()) < n*self.train[col].std()]\n",
    "\n",
    "    def eliminar_outliers_p(self, df, col, p=0.001):\n",
    "        l,h = p if type(p) is list else [p,1-p]\n",
    "        q_low = df[col].quantile(l)\n",
    "        q_hi  = df[col].quantile(h)\n",
    "        return df[(df[col] < q_low) | (df[col] > q_hi) ]\n",
    "    \n",
    "    def convertir_fechas(self, cols_date):\n",
    "        for df in [self.train, self.test]:\n",
    "            for date_col in cols_date:\n",
    "                df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    def ampliar_info_fechas(self, cols_date):\n",
    "        for df in [self.train, self.test]:\n",
    "            for x in cols_date:\n",
    "                df[x+'_dow'] = df[x].dt.dayofweek \n",
    "                df[x+'_moy'] = df[x].dt.month \n",
    "\n",
    "\n",
    "    def agregar_datos_de_cuentas(self):\n",
    "        part_win = self.train[self.train.Stage=='Closed Won'].groupby(['Account_Name']).Stage.count()\n",
    "        part = self.train.groupby(['Account_Name']).Stage.count()\n",
    "        accounts = pd.concat([part,part_win],axis=1)\n",
    "        accounts.columns = ['participo','gano']\n",
    "        accounts.fillna(0,inplace=True)\n",
    "        accounts['gano_p'] = accounts.gano/accounts.participo\n",
    "        accounts\n",
    "        for c in accounts.index:\n",
    "            for df in [self.train,self.test]:\n",
    "                df.loc[df.Account_Name==c,'participo'] = accounts.loc[c,'participo']\n",
    "                df.loc[df.Account_Name==c,'gano'] = accounts.loc[c,'gano']\n",
    "                df.loc[df.Account_Name==c,'gano_p'] = accounts.loc[c,'gano_p']\n",
    "\n",
    "    def agregar_datos_de_fechas(self):\n",
    "        for df in [self.train,self.test]:\n",
    "            df['f_days_from_client_created'] = (df['Opportunity_Created_Date'] - df['Account_Created_Date']).dt.days\n",
    "            # dias desde la ultima actividad\n",
    "            df['f_days_from_last_activity'] = (df['Last_Modified_Date'] - df['Opportunity_Created_Date']).dt.days\n",
    "            # dias que dura el presupuesto\n",
    "            df['f_days_expiry_duration'] = (df['Quote_Expiry_Date'] - df['Opportunity_Created_Date']).dt.days \n",
    "\n",
    "\n",
    "            df['planned_days'] = (df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']).dt.days\n",
    "            #agrupamientos\n",
    "            tg = df.groupby('Opportunity_ID')\n",
    "            df['Total_Amount_sum'] = tg.Total_Amount.transform('sum')\n",
    "\n",
    "            df['planned_days_mean'] = tg.planned_days.transform('mean')\n",
    "            df['planned_days_median'] = tg.planned_days.transform('median')\n",
    "            df['planned_days_max'] = tg.planned_days.transform('max')\n",
    "            df['planned_days_min'] = tg.planned_days.transform('min')\n",
    "\n",
    "            df['min_planned_start_Date'] = tg.Planned_Delivery_Start_Date.transform('min')\n",
    "            df['max_planned_end_Date'] = tg.Planned_Delivery_End_Date.transform('max')\n",
    "            df['planned_g_diff'] = (df['max_planned_end_Date'] - df['min_planned_start_Date']).dt.days\n",
    "        \n",
    "        \n",
    "Ana = Analizador(train,test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizo de que manera agrupa \"Opportunity_ID\", que es el atributo que tengo que estimar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuales cambian en train\n",
      "Index(['ID', 'Product_Family', 'Product_Name', 'ASP', 'ASP_(converted)',\n",
      "       'Planned_Delivery_Start_Date', 'Planned_Delivery_End_Date', 'Month',\n",
      "       'Delivery_Quarter', 'Delivery_Year', 'TRF', 'Total_Amount'],\n",
      "      dtype='object')\n",
      "cuales cambian en test\n",
      "Index(['ID', 'Product_Family', 'Product_Name', 'ASP', 'ASP_(converted)',\n",
      "       'Planned_Delivery_Start_Date', 'Planned_Delivery_End_Date', 'Month',\n",
      "       'Delivery_Quarter', 'Delivery_Year', 'TRF', 'Total_Amount'],\n",
      "      dtype='object')\n",
      "Valores unicos de Opportunity_ID\n",
      "9841\n"
     ]
    }
   ],
   "source": [
    "print('cuales cambian en train')\n",
    "print(train.columns[(train.groupby('Opportunity_ID').nunique() > 1).any()])\n",
    "print('cuales cambian en test')\n",
    "print(test.columns[(test.groupby('Opportunity_ID').nunique() > 1).any()])\n",
    "\n",
    "print(\"Valores unicos de Opportunity_ID\")\n",
    "print(train.groupby('Opportunity_ID').count().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambian las siguientes:\n",
    "\n",
    "* Product_Family\n",
    "* Product_Name\n",
    "* ASP\n",
    "* ASP_(converted)\n",
    "* Planned_Delivery_Start_Date\n",
    "* Planned_Delivery_End_Date\n",
    "* Month\n",
    "* Delivery_Quarter\n",
    "* Delivery_Year\n",
    "* TRF\n",
    "* Total_Amount\n",
    "\n",
    "Analizo estas columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminia las columnas de [test y train] que no aportan informacion en train.\n",
      "Index(['Submitted_for_Approval', 'Last_Activity', 'ASP_(converted)_Currency',\n",
      "       'Actual_Delivery_Date', 'Prod_Category_A'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "Ana.eliminar_columnas_que_no_cambian()\n",
    "\n",
    "cols_date = [x for x in train.columns if x.endswith('_Date')]\n",
    "Ana.convertir_fechas(cols_date)\n",
    "Ana.ampliar_info_fechas(cols_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Region', 'Territory', 'Pricing, Delivery_Terms_Quote_Appr',\n",
       "       'Pricing, Delivery_Terms_Approved', 'Bureaucratic_Code_0_Approval',\n",
       "       'Bureaucratic_Code_0_Approved', 'Bureaucratic_Code',\n",
       "       'Account_Created_Date', 'Source ', 'Billing_Country', 'Account_Name',\n",
       "       'Opportunity_Name', 'Opportunity_ID', 'Sales_Contract_No',\n",
       "       'Account_Owner', 'Opportunity_Owner', 'Account_Type',\n",
       "       'Opportunity_Type', 'Quote_Type', 'Delivery_Terms',\n",
       "       'Opportunity_Created_Date', 'Brand', 'Product_Type', 'Size',\n",
       "       'Product_Category_B', 'Price', 'Currency', 'Quote_Expiry_Date',\n",
       "       'Last_Modified_Date', 'Last_Modified_By', 'Product_Family',\n",
       "       'Product_Name', 'ASP_Currency', 'ASP', 'ASP_(converted)',\n",
       "       'Planned_Delivery_Start_Date', 'Planned_Delivery_End_Date', 'Month',\n",
       "       'Delivery_Quarter', 'Delivery_Year', 'TRF', 'Total_Amount_Currency',\n",
       "       'Total_Amount', 'Total_Taxable_Amount_Currency', 'Total_Taxable_Amount',\n",
       "       'Stage', 'Account_Created_Date_dow', 'Account_Created_Date_moy',\n",
       "       'Opportunity_Created_Date_dow', 'Opportunity_Created_Date_moy',\n",
       "       'Quote_Expiry_Date_dow', 'Quote_Expiry_Date_moy',\n",
       "       'Last_Modified_Date_dow', 'Last_Modified_Date_moy',\n",
       "       'Planned_Delivery_Start_Date_dow', 'Planned_Delivery_Start_Date_moy',\n",
       "       'Planned_Delivery_End_Date_dow', 'Planned_Delivery_End_Date_moy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ana.train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revisar Año max 2208 en Planned_Delivery_End_Date en train\n",
      "Corrijo fecha\n"
     ]
    }
   ],
   "source": [
    "print('Revisar Año max 2208 en Planned_Delivery_End_Date en train')\n",
    "train[train['Planned_Delivery_End_Date'] > pd.to_datetime('2022-01-01') ]['Planned_Delivery_End_Date']\n",
    "\n",
    "print('Corrijo fecha')\n",
    "train.at[15370,'Planned_Delivery_End_Date'] = pd.to_datetime('2018-12-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train,test]:\n",
    "    df.Sales_Contract_No = pd.to_numeric(df.Sales_Contract_No, errors='coerce')\n",
    "    df['con_contrato'] = df.Sales_Contract_No > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con_contrato\n",
      "Stage                      \n",
      "Closed Lost         579.000\n",
      "Closed Won        9,394.000\n",
      "Negotiation           0.000\n",
      "Proposal              0.000\n",
      "Qualification         0.000\n",
      "El atributo \"Sales_Contract_nro\", evidentemente esta ligado al Closed Won. Probablemente, se asigne este numero, cuando se concreta la venta. Por lo tanto no se usa como feature \n"
     ]
    }
   ],
   "source": [
    "print(train[['con_contrato','Stage']].groupby('Stage').sum())\n",
    "print('El atributo \"Sales_Contract_nro\", evidentemente esta ligado al Closed Won. Probablemente, se asigne este numero, cuando se concreta la venta. Por lo tanto no se usa como feature ')\n",
    "for df in [train,test]:\n",
    "    df.drop(['con_contrato','Sales_Contract_No'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimino las fechas\n",
    "for df in [train,test]:\n",
    "    df.drop(cols_date, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-241-b5175bb96445>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-241-b5175bb96445>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    NO ELIMINA NADA ????\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "a = Ana.eliminar_all_outliers(1)\n",
    "print(train.shape)\n",
    "#An.eliminar_all_outliers(4)\n",
    "NO ELIMINA NADA ????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revisar estos casos\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Total_Taxable_div'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-fc95759a49c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'revisar estos casos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtax_mayor_a_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTotal_Taxable_div\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTotal_Taxable_Amount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtax_mayor_a_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Opportunity_ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Total_Taxable_Amount'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Total_Amount_sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Total_Taxable_div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Stage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Total_Taxable_div'"
     ]
    }
   ],
   "source": [
    "print('revisar estos casos')\n",
    "tax_mayor_a_total = train[(np.abs(train.Total_Taxable_div) > 2) & train.Total_Taxable_Amount > 0]\n",
    "tax_mayor_a_total[['Opportunity_ID','Total_Taxable_Amount','Total_Amount_sum','Total_Taxable_div','Stage']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_fijas_train = train.columns[(train.groupby('Opportunity_ID').nunique() <= 1).all()]\n",
    "col_fijas_test = test.columns[(test.groupby('Opportunity_ID').nunique() <= 1).all()]\n",
    "df_train = train[col_fijas_train].drop_duplicates().set_index('Opportunity_ID')\n",
    "df_test = test[col_fijas_test].drop_duplicates().set_index('Opportunity_ID')\n",
    "\n",
    "#df_train = train.set_index('Opportunity_ID')\n",
    "#df_test = test.set_index('Opportunity_ID')\n",
    "\n",
    "#Elimino identificadoes\n",
    "#cols_to_drop =  ['ID', 'Opportunity_ID', 'Opportunity_Name']\n",
    "#df_train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "#df_test.drop(cols_to_drop, axis=1, inplace=True)\n",
    "#col_fijas_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('output/train_minable.csv', index=False)\n",
    "#test.to_csv('output/test_minable.csv', index=False)\n",
    "#df_train.to_csv('output/df_train_minable.csv')\n",
    "#df_test.to_csv('output/df_test_minable.csv')\n",
    "\n",
    "#df = pd.concat([df_test,df_train], axis=0, ignore_index=True)\n",
    "#df.loc[df.Stage.isna(),'Stage'] = 'FALTA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed Won       5072\n",
      "Closed Lost      4719\n",
      "Proposal           30\n",
      "Negotiation        11\n",
      "Qualification       9\n",
      "Name: Stage, dtype: int64\n",
      "Closed Won     5072\n",
      "Closed Lost    4719\n",
      "Name: Stage, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#solo dejo los Stage Win y Lost\n",
    "print(df_train.Stage.value_counts())\n",
    "df_train = df_train[df_train.Stage.isin(['Closed Won','Closed Lost'])]\n",
    "print(df_train.Stage.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "An.agregar_datos_de_cuentas(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop('Stage', axis=1)\n",
    "#y = df_train['Stage'].apply(lambda x: 1 if x == 'Closed Won' else 0)\n",
    "y = df_train['Stage']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3)\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#features_to_ignore=['con_contrato','Sales_Contract_No']\n",
    "#for df in [X_train, X_valid, X_test]:\n",
    "#    df.drop(features_to_ignore, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object','bool']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8488767869298843"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#cls_rf = RandomForestClassifier(n_estimators=150, criterion='gini')\n",
    "cls_rf = RandomForestClassifier()\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', cls_rf)])\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_valid)\n",
    "rf.score(X_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pricing, Delivery_Terms_Quote_Appr',\n",
       "       'Pricing, Delivery_Terms_Approved', 'Bureaucratic_Code_0_Approval',\n",
       "       'Bureaucratic_Code_0_Approved', 'Total_Taxable_Amount',\n",
       "       'Account_Created_Date_dow', 'Account_Created_Date_moy',\n",
       "       'Opportunity_Created_Date_dow', 'Opportunity_Created_Date_moy',\n",
       "       'Quote_Expiry_Date_dow', 'Quote_Expiry_Date_moy',\n",
       "       'Last_Modified_Date_dow', 'Last_Modified_Date_moy', 'participo', 'gano',\n",
       "       'gano_p'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features\n",
    "#categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8787759131293189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "cls_dt = DecisionTreeClassifier()\n",
    "dt = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', cls_dt)\n",
    "    ])\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_valid)\n",
    "print(dt.score(X_valid, y_valid))\n",
    "\n",
    "#tree.export_text(cls_dt,max_depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=0.025, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "model score: 0.791\n",
      "------------------------------------------------------------\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "model score: 0.879\n",
      "------------------------------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "model score: 0.915\n",
      "------------------------------------------------------------\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "model score: 0.823\n",
      "------------------------------------------------------------\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "model score: 0.859\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "pipes=[]\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    #NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)  \n",
    "    pipes.append(pipe) \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_valid, y_valid))\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 1220.71 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.914 (std: 0.004)\n",
      "Parameters: {'classifier__n_estimators': 190, 'classifier__criterion': 'gini'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.914 (std: 0.005)\n",
      "Parameters: {'classifier__n_estimators': 170, 'classifier__criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.914 (std: 0.003)\n",
      "Parameters: {'classifier__n_estimators': 150, 'classifier__criterion': 'entropy'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls_model=pipes[2]\n",
    "\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': range(10,200,20),\n",
    "    #'classifier__max_depth': range(10,170,10),\n",
    "    #'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #'classifier__max_depth' : [4,5,6,7,8],\n",
    "    'classifier__criterion' :['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from time import time\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\n",
    "            'n_estimator': [100, 200, 400, 600],\n",
    "            #  'l1_ratio': stats.uniform(0, 1),\n",
    "            #  'alpha': loguniform(1e-4, 1e0)\n",
    "              }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(cls_model, param_distributions=param_grid,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE scores:\n",
      " [-0.89141165 -0.87857848 -0.85982231 -0.88647581 -0.89239882]\n"
     ]
    }
   ],
   "source": [
    "#cls_model=CV.best_estimator_\n",
    "cls_model=rf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(cls_model, X_valid, y_valid,\n",
    "                              cv=5,\n",
    "                              #scoring='neg_mean_absolute_error'\n",
    "                              )\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2551, 2)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_y = cls_model.predict_proba(X_test)\n",
    "df = pd.concat([pd.Series(X_test.index),  pd.Series(X_test_y[:,1])], axis=1)\n",
    "df.shape\n",
    "df.to_csv('output/p7.csv', index=False, header=False)\n",
    "#df_train.to_csv('output/p7_train.csv', index=False)\n",
    "#df_test.to_csv('output/p7_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_y = cls_model.predict_proba(X_test)\n",
    "df = pd.concat([pd.Series(X_test.index),  pd.Series(X_test_y[:,1])], axis=1)\n",
    "df = df.groupby('Opportunity_ID').mean()\n",
    "df\n",
    "df.to_csv('output/p7b.csv', index=True, header=False)\n",
    "#df_train.to_csv('output/p7_train.csv', index=False)\n",
    "#df_test.to_csv('output/p7_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
